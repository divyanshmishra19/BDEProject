/*
 * Created by Jacob Summers on 2023.3.30
 * Copyright Â© 2023 Jacob Summers. All rights reserved.
 * Inspired by https://stackoverflow.com/questions/24771823/spark-streaming-accumulated-word-count
 * and https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala
 */
import org.apache.log4j.{Level, LogManager}
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import provenance.util.Provenance.logProvenance
import provenance.util.WordCountOperationLineNumber.getOperationLineNumber
import provenance.util.{MapOperation, ProvenanceDStream, ProvenanceReceiverInputDStream, SplitOperation, UpdateStateByKeyOperation}

object WordCount {
  def main(args: Array[String]): Unit = {

    val logger = LogManager.getLogger("org.apache.spark")
    logger.setLevel(Level.ERROR)

    //comment this out if not on windows
    System.setProperty("hadoop.home.dir", "C:\\hadoop")

    val sparkConf = new SparkConf().setAppName("SpectraWordCount")
      .setMaster("local[*]")
      .set("spark.executor.memory", "512m")
      .set("spark.driver.extraJavaOptions", "-Dlog4j.configuration=none")
      .set("spark.driver.allowMultipleContexts", "true")

    //connection to spark container
    //.setMaster("spark://localhost:7077")
    // Create the context with a 10 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(10))
    ssc.checkpoint("checkpoint")

    // Create a NetworkInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val splitOperation = SplitOperation(" ", _.split(" "))
    val lines = ProvenanceReceiverInputDStream(ssc.socketTextStream("localhost", 9999), "Source: SocketTextStream")
    val words = splitOperation(lines)


    val mapOperation1 = MapOperation[String, (String, Int)]("x => (x, 1)", x => (x, 1))
    val mapOperation2 = MapOperation[String, (String, Int)]("x => (x, 10)", x => (x, 10))

    var sExists = false
    words.dStream.foreachRDD { rdd =>
      rdd.collect.foreach { word =>
        if (word.startsWith("s")) {
          sExists = true
        }
      }
    }

    val wordDstream = if (sExists) {
                        mapOperation2(words)
                      } else {
                        mapOperation1(words)
                      }

    val updateFunc = (values: Seq[Int], state: Option[Int]) => {
      val currentCount = values.foldLeft(0)(_ + _)

      val previousCount = state.getOrElse(0)

      Some(currentCount + previousCount)
    }


    val updateStateByKeyOperation = UpdateStateByKeyOperation[String, Int, Int]("word count update", updateFunc)
    val stateDstream = updateStateByKeyOperation(wordDstream)
    // Log provenance information for both operations
    logProvenance(stateDstream)

    // Update the cumulative count using updateStateByKey
    // This will give a Dstream made of state (which is the cumulative count of the words)
    stateDstream.dStream.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
